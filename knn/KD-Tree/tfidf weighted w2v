from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
import scikitplot.metrics as skplt
from sklearn.metrics import auc

y=filtered_data['Score'][:25000]
df_2=pd.read_pickle('preprocess_reviews')
x=df_2[:25000]

x_tr, x_cv, y_tr, y_cv = train_test_split(x, y, test_size=0.2)
x_train, x_test, y_train, y_test = train_test_split(x_tr, y_tr, test_size=0.2)

#X_train

model_train = TfidfVectorizer()
tf_idf_matrix = model_train.fit_transform(x_train)
# we are converting a dictionary with word as a key, and the idf as a value
dictionary_train = dict(zip(model_train.get_feature_names(), list(model_train.idf_)))
    
# TF-IDF weighted Word2Vec
tfidf_feat_train = model_train.get_feature_names() # tfidf words/col-names
# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf

tfidf_sent_vectors_train = []; # the tfidf-w2v for each sentence/review is stored in this list

list_of_sentence_train=[]
for i in x_train:
    list_of_sentence_train.append(i.split())
    
for sent in tqdm(list_of_sentence_train): # for each review/sentence 
    sent_vec = np.zeros(50) # as word vectors are of zero length
    weight_sum =0; # num of words with a valid vector in the sentence/review
    for word in sent: # for each word in a review/sentence
        if word in w2v_words_train and word in tfidf_feat_train:
            vec = w2v_model_train.wv[word]
#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]
            # to reduce the computation we are 
            # dictionary[word] = idf value of word in whole courpus
            # sent.count(word) = tf valeus of word in this review
            tf_idf = dictionary_train[word]*(sent.count(word)/len(sent))
            sent_vec += (vec * tf_idf)
            weight_sum += tf_idf
    if weight_sum != 0:
        sent_vec /= weight_sum
    tfidf_sent_vectors_train.append(sent_vec)

# X_CV

model_cv = TfidfVectorizer()
tf_idf_matrix = model_cv.fit_transform(x_cv)
# we are converting a dictionary with word as a key, and the idf as a value
dictionary_cv = dict(zip(model_cv.get_feature_names(), list(model_cv.idf_)))
    
# TF-IDF weighted Word2Vec
tfidf_feat_cv = model_cv.get_feature_names() # tfidf words/col-names
# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf

tfidf_sent_vectors_cv = []; # the tfidf-w2v for each sentence/review is stored in this list

list_of_sentence_cv=[]
for i in x_cv:
    list_of_sentence_cv.append(i.split())
    
for sent in tqdm(list_of_sentence_cv): # for each review/sentence 
    sent_vec = np.zeros(50) # as word vectors are of zero length
    weight_sum =0; # num of words with a valid vector in the sentence/review
    for word in sent: # for each word in a review/sentence
        if word in w2v_words_cv and word in tfidf_feat_cv:
            vec = w2v_model_cv.wv[word]
#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]
            # to reduce the computation we are 
            # dictionary[word] = idf value of word in whole courpus
            # sent.count(word) = tf valeus of word in this review
            tf_idf = dictionary_cv[word]*(sent.count(word)/len(sent))
            sent_vec += (vec * tf_idf)
            weight_sum += tf_idf
    if weight_sum != 0:
        sent_vec /= weight_sum
    tfidf_sent_vectors_cv.append(sent_vec)

#x_test

model_test = TfidfVectorizer()
tf_idf_matrix = model_test.fit_transform(x_test)
# we are converting a dictionary with word as a key, and the idf as a value
dictionary_test = dict(zip(model_test.get_feature_names(), list(model_test.idf_)))
    
# TF-IDF weighted Word2Vec
tfidf_feat_test = model_test.get_feature_names() # tfidf words/col-names
# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf

tfidf_sent_vectors_test = []; # the tfidf-w2v for each sentence/review is stored in this list

list_of_sentence_test=[]
for i in x_test:
    list_of_sentence_test.append(i.split())
    
for sent in tqdm(list_of_sentence_test): # for each review/sentence 
    sent_vec = np.zeros(50) # as word vectors are of zero length
    weight_sum =0; # num of words with a valid vector in the sentence/review
    for word in sent: # for each word in a review/sentence
        if word in w2v_words_test and word in tfidf_feat_test:
            vec = w2v_model_test.wv[word]
#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]
            # to reduce the computation we are 
            # dictionary[word] = idf value of word in whole courpus
            # sent.count(word) = tf valeus of word in this review
            tf_idf = dictionary_test[word]*(sent.count(word)/len(sent))
            sent_vec += (vec * tf_idf)
            weight_sum += tf_idf
    if weight_sum != 0:
        sent_vec /= weight_sum
    tfidf_sent_vectors_test.append(sent_vec)

#print(len(x_train), len(x_test), len(x_cv))

cv_acc_list = []
train_acc_list=[]
i_value=[]
for i in range(1,45,2):
    neigh=KNeighborsClassifier(n_neighbors=i, algorithm='kd_tree', n_jobs=-1)
    neigh.fit(tfidf_sent_vectors_train, y_train)
    pred_train=neigh.predict_proba(tfidf_sent_vectors_train)[:,1]
    pred_cv=neigh.predict_proba(tfidf_sent_vectors_cv)[:,1]
    kn=roc_auc_score(y_cv, pred_cv)
    tn=roc_auc_score(y_train, pred_train)
    cv_acc_list.append(kn)
    train_acc_list.append(tn)
    i_value.append(i)
    #print('for k={l} accuracy is {j}'.format(l=i, j=kn))
    
# ploting graph of accuracy wrt k
plt.title('Accuracy of train and CV wrt K')
plt.plot(i_value, cv_acc_list, label='cv_data')
plt.plot(i_value, train_acc_list, label='Train_data')
plt.legend()
plt.xlabel('K value')
plt.ylabel('Accuracy')
plt.show()

print('we are getting maximum accuracy of {x} on k={y} for cv data'.format(x=max(cv_acc_list), y=i_value[cv_acc_list.index(max(cv_acc_list))]))
print("now let's find accuracy on test data for k={x}".format(x=i_value[cv_acc_list.index(max(cv_acc_list))]))

# finding auc of test data.
neighbor=KNeighborsClassifier(n_neighbors=i_value[cv_acc_list.index(max(cv_acc_list))], algorithm='kd_tree', n_jobs=-1)
neighbor.fit(tfidf_sent_vectors_train, y_train)
pred_test=neighbor.predict_proba(tfidf_sent_vectors_test)[:,1]
pred_test_1=neighbor.predict(tfidf_sent_vectors_test)
pred_train_1=neighbor.predict_proba(tfidf_sent_vectors_train)[:,1]
auc_test = roc_auc_score(y_test, pred_test)
#print('-'*20, 'finding accuracy', '-'*20)
#print('we got accuracy of {x} by k={y} on test data'.format(x=auc_test, y=i_value[cv_acc_list.index(max(cv_acc_list))]))

#kd_tree_w_tfidf_k = i_value[cv_acc_list.index(max(cv_acc_list))]


# plotting Roc curve of test data
#https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/

fpr_train, tpr_train, threshold= roc_curve(y_train, pred_train_1)
fpr_test, tpr_test, threshold = roc_curve(y_test, pred_test)
plt.plot([0,1], [0,1], linestyle='--')
plt.plot(fpr_test, tpr_test, marker='.', label='Test_AUC'+str(auc(fpr_test, tpr_test)))
plt.plot(fpr_train, tpr_train, marker='.', label='Train_AUC'+str(auc(fpr_train, tpr_train)))
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

kd_tree_w_tfidf_auc=auc(fpr_test, tpr_test))

# confusion matrics

skplt.plot_confusion_matrix(y_test, pred_test_1);

